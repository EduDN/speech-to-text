{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFGaslWwhgbQ"
      },
      "source": [
        "**Instalación Whisper**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE5TFw_CnQgV"
      },
      "source": [
        "## DESCARGA DE AUDIO DE LLAMADA TELEFONICA DE YOUTUBE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx-LSHrjmv4x",
        "outputId": "4ac62755-9a69-4fa0-ef0e-1dfaa73e1f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.10.7-py3-none-any.whl.metadata (171 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.3/171.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.8.30)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.2.3)\n",
            "Collecting websockets>=13.0 (from yt-dlp)\n",
            "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.32.2->yt-dlp) (3.10)\n",
            "Downloading yt_dlp-2024.10.7-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, websockets, pycryptodomex, mutagen, yt-dlp\n",
            "Successfully installed brotli-1.1.0 mutagen-1.47.0 pycryptodomex-3.21.0 websockets-13.1 yt-dlp-2024.10.7\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSSxkZjRmWk_",
        "outputId": "9a33193a-0af0-4c87-a99c-735dd818c147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=G0KisoxpgnA\n",
            "[youtube] G0KisoxpgnA: Downloading webpage\n",
            "[youtube] G0KisoxpgnA: Downloading ios player API JSON\n",
            "[youtube] G0KisoxpgnA: Downloading mweb player API JSON\n",
            "[youtube] G0KisoxpgnA: Downloading player 34059770\n",
            "[youtube] G0KisoxpgnA: Downloading m3u8 information\n",
            "[info] G0KisoxpgnA: Downloading 1 format(s): 251\n",
            "[download] Destination: /content/sample_data/Vendedora telefónica atiende a un cliente difícil.webm\n",
            "[download] 100% of    2.98MiB in 00:00:00 at 6.93MiB/s   \n",
            "[ExtractAudio] Destination: /content/sample_data/Vendedora telefónica atiende a un cliente difícil.mp3\n",
            "Deleting original file /content/sample_data/Vendedora telefónica atiende a un cliente difícil.webm (pass -k to keep)\n",
            "Descarga completada.\n"
          ]
        }
      ],
      "source": [
        "import yt_dlp\n",
        "import os\n",
        "\n",
        "def descargar_audio(url, output_path):\n",
        "    ydl_opts = {\n",
        "        'format': 'bestaudio/best',\n",
        "        'postprocessors': [{\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'mp3',\n",
        "            'preferredquality': '192',\n",
        "        }],\n",
        "        'outtmpl': os.path.join(output_path, '%(title)s.%(ext)s'),\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "\n",
        "# Ejemplo de uso\n",
        "url_video = \"https://www.youtube.com/watch?v=G0KisoxpgnA\"\n",
        "carpeta_salida = \"/content/sample_data\"\n",
        "\n",
        "# Asegúrate de que la carpeta de salida exista\n",
        "if not os.path.exists(carpeta_salida):\n",
        "    os.makedirs(carpeta_salida)\n",
        "\n",
        "descargar_audio(url_video, carpeta_salida)\n",
        "print(\"Descarga completada.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ah3HTand1G"
      },
      "source": [
        "## Speech to text Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkUzq0QnhfqW",
        "outputId": "2c2426a9-a23b-47fd-94bd-f0866b3bad59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803321 sha256=56dbd417ca5d7b652154ab8a98a71c5693f3359169823067a01e18d2fc6cd012\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.0.0\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-gg2r14je\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-gg2r14je\n",
            "  Resolved https://github.com/openai/whisper.git to commit 25639fc17ddc013d56c594bfbf7644f2185fad84\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20240930) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install -U openai-whisper\n",
        "%pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knNADlwShmMs",
        "outputId": "51f1625a-4426-4800-93a9-833677b5c645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,030 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,590 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,159 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,371 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,318 kB]\n",
            "Fetched 15.9 MB in 4s (4,004 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 54 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4P1fn_chvM3"
      },
      "source": [
        "## Proyecto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETZRl1DDhxaP",
        "outputId": "e0e3e8a1-afe0-45a9-dc2e-4730722f59b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:39<00:00, 77.5MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ... ... ... ... ¿Aló? Buenos días, ¿Usted es el titular de la línea? Sí, sí, sí, soy yo mismo. ¿Me puede decir su nombre, por favor? ¿Gabriel? Mire, señor Gabriel, le llamo de Moviselgo para ofrecerle una promoción. Consiste en la instalación de una línea adicional en su casa, donde usted tendrá derecho... Perdón, señorita, pero es que, ¿exactamente quién es usted? Mi nombre es Marcela Restrepo de Moviselgo y estamos llamando... Sí, sí, Marcela, discúlpeme, pero para nuestra seguridad me gustaría comprobar algunos datos antes de continuar con esta conversación. ¿Le importa? No, no tengo problema, señor. ¿Desde qué teléfono me llama? Es que en la pantallita aquí del mío sale número privado. El interno mío es 1004. Ya, ¿y para qué departamento de Moviselgo trabaja usted? Telemarketing Activo. Perdón, ¿me podría dar su identificación de trabajadora de Moviselgo? Señor, disculpe, pero creo que toda esa información no es necesaria. Entonces, pues lamentablemente voy a tener que colgar porque es que no tengo la seguridad de hablar con una trabajadora de Moviselgo. Pero yo le puedo garantizar... Mira, Marcela, cada vez que yo llamo a Moviselgo antes de poder comenzar cualquier trámite estoy obligado a dar mis datos al recepcionista, la secretaria, el auxiliar que me pone a escuchar musiquita mientras piensa quién le chuta la pelota, y luego repetirle a la historia al analista de ventas, al analista de quejas y reclamos, al analista de solicitudes nuevas, al de solicitudes viejas, al de solicitudes no atendidas, al de solicitudes ya atendidas, y al analista nuevo que apenas está entrenando y no tiene ni idea de qué le estoy preguntando. Así que si usted quiere hablar conmigo, pues tendrá que identificarse. Está bien, señor. Para que esté más tranquilo, le informo que mi número de empleada es el 3459-2B2. Un momentico mientras lo verifico. No se retire, Marcela. Señor. Un momento, es que toda la gente se encuentra ocupada aquí en la casa. Sí. Pero... ¿Hola? ¿Señor? Señor. ¿Aló? Marcela, Marcela, un momento, por favor, que toda la gente se encuentra aquí ocupada. Bueno. ¿Don Gabriel? ¿Don Gabriel? Aló, sí, Marcela. Gracias por la espera. Nuestros sistemas están un poco lentos hoy. ¿Cuál era el asunto de su llamada, perdón? Lo llamo de Moiselgo y estamos llamando para ofrecerle... ¿Qué? Nuestra promoción línea adicional en la que usted tiene derecho al uso de otra línea a muy bajo costo. ¿Usted estaría interesado en Gabriel? A ver, Marcela, gracias por llamar. Le voy a comunicar con mi mujer, que es la encargada de la sección de adquisición de productos técnicos en la casa. Por favor, no se retire de la línea. Ay, disculpe por la espera. ¿Me puede decir su teléfono? Es que en mi pantallita solo aparece el número privado. 1004. Gracias. ¿Y con quién estoy hablando? Ay, con Marcela. Marcela, ¿qué? Perdón. Marcela Restrepo. Ay, no. Perdón, ¿y cuál es su identificación de trabajadora de Moiselgo? 35-59-1212. Ah, no. Gracias por la información, Marcela. ¿Y en qué la puedo ayudar? Mire, la llamo de Moiselgo. Estamos llamando para ofrecerle nuestra promoción línea adicional en la que usted tiene derecho a otra línea. ¿Estaría interesada? Pues voy a ingresar su solicitud en nuestro programa de nuevas adquisiciones y dentro de algunos días nos contactamos con usted. O sea, ¿puede anotar el número de ingreso al programa, por favor? Hola. Hola. Sí. Sí. Sí. Sí. Sí. y\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU; using FP32 instead\")\n",
        "\n",
        "# Configurar para usar GPU\n",
        "model = whisper.load_model(\"large\", device=\"cuda\")\n",
        "result = model.transcribe(\"/content/sample_data/Vendedora telefónica atiende a un cliente difícil.mp3\")\n",
        "print(result[\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UERkGJcslYg"
      },
      "source": [
        "**Tienes que cambiar el entorno a gpu,**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "I4e1rg-UoUaR",
        "outputId": "31a6b0cb-4311-4f12-ce7d-47df73f303a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resumen generado:\n",
            "```json\n",
            "{\n",
            "  \"resumen\": \"La llamada es de una representante de Moviselgo que ofrece una promoción de línea adicional a Gabriel. Sin embargo, Gabriel inicialmente duda de la identidad de la representante y solicita su identificación. La representante proporciona su número de empleada (3459-2B2) y explica que está verificando los datos para garantizar la seguridad. Gabriel acepta proporcionar sus datos si la representante se identifica. La llamada continúa con la representante repitiendo los detalles de la promoción y solicitando la autorización de Gabriel para ingresar su solicitud en el programa de nuevas adquisiciones. Al final de la llamada, Gabriel pregunta por el número de ingreso al programa.\",\n",
            "  \"sentimiento\": \"neutral\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Configura la biblioteca\n",
        "genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))\n",
        "\n",
        "def resumir_texto(texto):\n",
        "    # Verifica si la API key está configurada\n",
        "    if not os.environ.get('GOOGLE_API_KEY'):\n",
        "        raise ValueError(\"La API key de Google no está configurada. Por favor, configura la variable de entorno GOOGLE_API_KEY.\")\n",
        "\n",
        "    # Crea un modelo\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "    # Prepara el prompt\n",
        "    prompt = f\"\"\"Analiza el siguiente texto y proporciona:\n",
        "    1. Un resumen conciso que capture las ideas principales y los puntos clave de la llamada.\n",
        "    2. El sentimiento general de la llamada (positivo, negativo o neutral).\n",
        "    Texto a analizar:\n",
        "    {texto}\n",
        "    Devuelve el análisis en formato JSON con las siguientes claves:\n",
        "    \"resumen\" y \"sentimiento\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Genera el resumen\n",
        "    response = model.generate_content(prompt)\n",
        "    \n",
        "    # Parsea la respuesta JSON\n",
        "    try:\n",
        "        resultado = json.loads(response.text)\n",
        "    except json.JSONDecodeError:\n",
        "        resultado = {\"error\": \"No se pudo parsear la respuesta como JSON\"}\n",
        "\n",
        "    return resultado\n",
        "\n",
        "# Ejemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    # Texto de ejemplo para resumir\n",
        "    text = \"Tu texto aquí\"  # Reemplaza esto con tu variable 'result'\n",
        "\n",
        "    try:\n",
        "        # Llamada a la función para resumir el texto\n",
        "        resumen = resumir_texto(text)\n",
        "        print(\"Resumen generado:\")\n",
        "        print(json.dumps(resumen, indent=2, ensure_ascii=False))\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iVA7Do-pFmV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
